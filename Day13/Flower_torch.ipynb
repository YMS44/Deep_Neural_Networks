{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2e75ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62893468",
   "metadata": {},
   "source": [
    "# Image Processing with Neural Network\n",
    "## Working with PyTorch - Flowers DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d52e7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "###-----------------\n",
    "### Import Libraries\n",
    "###-----------------\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torchsummary import summary\n",
    "from torchvision.transforms import v2\n",
    "from torchvision import datasets\n",
    "\n",
    "# from utils.helper import fn_plot_torch_hist, fn_plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b32a9af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "###----------------------\n",
    "### Some basic parameters\n",
    "###----------------------\n",
    "\n",
    "\n",
    "inpDir = 'D:\\DNN\\input' # location where input data is stored\n",
    "outDir = '../output' # location to store outputs\n",
    "modelDir = '../models'\n",
    "subDir = 'flower_photos'\n",
    "altName = 'cnn_base'\n",
    "\n",
    "RANDOM_STATE = 24 # for initialization ----- REMEMBER: to remove at the time of promotion to production\n",
    "torch.manual_seed(RANDOM_STATE) # Set Random Seed for reproducible  results\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "EPOCHS = 201 # number of epochs\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "ALPHA = 0.001 # learning rate\n",
    "\n",
    "TRAIN_SIZE = int(BATCH_SIZE*184) #\n",
    "TEST_SIZE = 0.2 \n",
    "\n",
    "# Input Image size\n",
    "IMG_HEIGHT = 186\n",
    "IMG_WIDTH = 186\n",
    "\n",
    "# for early stopping \n",
    "PATIENCE = 20\n",
    "LR_FACTOR = 0.1\n",
    "LR_PATIENCE = 10\n",
    "\n",
    "# parameters for Matplotlib\n",
    "params = {'legend.fontsize': 'medium',\n",
    "          'figure.figsize': (15, 6),\n",
    "          'axes.labelsize': 'medium',\n",
    "          'axes.titlesize':'large',\n",
    "          'xtick.labelsize':'medium',\n",
    "          'ytick.labelsize':'medium'\n",
    "         }\n",
    "\n",
    "CMAP = plt.cm.coolwarm\n",
    "\n",
    "plt.rcParams.update(params)\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid') # plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "283d4a35-dea2-4166-ac2b-74a96d38ca29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2944"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca08aaf-16ec-496e-9a84-48ed8b97ed10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dbd48bc7",
   "metadata": {},
   "source": [
    "## Basic Hygiene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3359ffd4-c5a6-474e-b9aa-c3ab328255b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.0+cpu'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ededc7c6-8502-4cc2-9538-aa8a35db6652",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.18.0+cpu'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchvision\n",
    "torchvision.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f53e9e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if all directories are present\n",
    "# if not os.path.exists(os.path.join(outDir)):\n",
    "#     os.makedirs(os.path.join(outDir))\n",
    "#     print ('Created {} directory'.format(outDir))\n",
    "\n",
    "# if not os.path.exists(os.path.join(modelDir, subDir)):\n",
    "#     os.makedirs(os.path.join(modelDir, subDir))\n",
    "#     print ('Created {} directory'.format(os.path.join(modelDir, subDir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da1a7385-64c5-41ff-acbf-71072a125f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_showImages(img, ax):\n",
    "\n",
    "    img = img.numpy().transpose((1,2,0))  #change to channel last\n",
    "\n",
    "    means = np.array([0.485, 0.456, 0.406])  #mean as per transforms\n",
    "    \n",
    "    stds = np.array([0.229, 0.224, 0.225])   #std as per transform\n",
    "    \n",
    "    img = stds * img + means     #undo normalization\n",
    "     \n",
    "    img = np.clip(img, 0, 1)     #clip values between [0,1]\n",
    "    \n",
    "    ax.imshow(img)\n",
    "    ax.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f76da2f-e7fd-4ef1-9ffe-f8442139528a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is CUDA available:  False\n",
      "CUDA version:  None\n"
     ]
    }
   ],
   "source": [
    "print ('Is CUDA available: ', torch.cuda.is_available())\n",
    "\n",
    "print ('CUDA version: ', torch.version.cuda )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b64d19bf-cf35-494e-8627-bb611eef0869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "# Get cpu or gpu device for training.\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591b1fc2",
   "metadata": {},
   "source": [
    "## Read Flowers data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2177d69",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transform' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 34\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;124;03mthis is where we will add data augmentation\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m     32\u001b[0m data_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(inpDir, subDir)\n\u001b[1;32m---> 34\u001b[0m dataset \u001b[38;5;241m=\u001b[39m datasets\u001b[38;5;241m.\u001b[39mImageFolder(data_dir, transform\u001b[38;5;241m=\u001b[39mtransform)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'transform' is not defined"
     ]
    }
   ],
   "source": [
    "###---------------\n",
    "### Transformation\n",
    "###---------------\n",
    "\n",
    "train_transform = v2.Compose([\n",
    "    v2.Resize(IMG_HEIGHT),\n",
    "    \n",
    "    #augmentation transformations\n",
    "    v2.RandomRotation(degrees=(-45, +45)),\n",
    "    v2.CenterCrop(IMG_HEIGHT),\n",
    "    v2.RandomHorizontalFlip(p=0.5),\n",
    "    v2.RandomVerticalFlip(p=0.5),\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale = True),\n",
    "    v2.Normalize( mean = [0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])  \n",
    "])\n",
    "\n",
    "test_transform = v2.Compose([\n",
    "    v2.Resize(IMG_HEIGHT),\n",
    "    v2.CenterCrop(IMG_HEIGHT),\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale = True),\n",
    "    v2.Normalize( mean = [0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])  ])\n",
    "\n",
    "\n",
    "'''\n",
    "this is where we will add data augmentation\n",
    "'''\n",
    "\n",
    "data_dir = os.path.join(inpDir, subDir)\n",
    "\n",
    "dataset = datasets.ImageFolder(data_dir, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276462a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label = next(iter(dataset))\n",
    "image.shape, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f08eda-e85a-42e5-9996-4e7c8d711c04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a237f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = len(train_dataset)\n",
    "\n",
    "indices = torch.randperm(dataset_size).tolist()\n",
    "\n",
    "train_dataset = torch.utils.data.Subset(train_dataset, indices[:TRAIN_SIZE])\n",
    "\n",
    "test_dataset = torch.utils.data.Subset(test_dataset, indices[TRAIN_SIZE:])\n",
    "\n",
    "len(train_dataset), len(test_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffbb79e-d3ac-44a4-a7ce-6087a617a382",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada03702-0708-4097-bd3a-d60eea23c5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.utils.data.DataLoader?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9b4805",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, \n",
    "                                               batch_size=BATCH_SIZE,\n",
    "                                               shuffle= True)\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, \n",
    "                                               batch_size=BATCH_SIZE,\n",
    "                                               shuffle= True)\n",
    "\n",
    "\n",
    "trainSteps = len(train_dataloader.dataset) // BATCH_SIZE\n",
    "\n",
    "testSteps = len(test_dataloader.dataset) // BATCH_SIZE\n",
    "\n",
    "print (trainSteps, testSteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcf1676-3ea7-4893-a934-bb891c5bd99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = {count: nClass for count, nClass in enumerate(train_dataset.dataset.classes)}\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fbfdbf-9b7f-407b-bc8e-0302a21157d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "numClasses = len(class_names)\n",
    "numClasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411e5cd8-30ec-4ca7-bdc5-9ca8548a86e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels =next(iter(train_dataloader))\n",
    "\n",
    "#print (images.shape, labels.shape)\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "for i in range (BATCH_SIZE):\n",
    "\n",
    "    ax = fig.add_subplot(BATCH_SIZE//8, 8, i+1)\n",
    "    \n",
    "    fn_showImages(images[i], ax)\n",
    "\n",
    "    ax.set_title(class_names[labels[i].item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1ba840",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels =next(iter(test_dataloader))\n",
    "\n",
    "#print (images.shape, labels.shape)\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "for i in range (BATCH_SIZE):\n",
    "\n",
    "    ax = fig.add_subplot(BATCH_SIZE//8, 8, i+1)\n",
    "    \n",
    "    fn_showImages(images[i], ax)\n",
    "\n",
    "    ax.set_title(class_names[labels[i].item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65933585-c812-408e-a764-62ab78ad5b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "\n",
    "    def __init__(self, numChannels, classes) :\n",
    "\n",
    "        #call parent constructor\n",
    "        super(LeNet, self).__init__()\n",
    "\n",
    "        #dropouts\n",
    "        dor1 = 0.3\n",
    "        dor2 = 0.3\n",
    "        dor3 = 0.4\n",
    "        dor4 = 0.5\n",
    "        dor5 = 0.5\n",
    "\n",
    "        #Define Layers\n",
    "        \n",
    "        # Set 1\n",
    "        out_channels1 = 32\n",
    "        # conv --> BN --> LeakyRelu --> Pool --> DO\n",
    "        self.conv1 = nn.Conv2d(in_channels = numChannels,\n",
    "                              out_channels = out_channels1,\n",
    "                              kernel_size = (3,3)) #output_shape 184, 184, 128\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm2d(out_channels1)\n",
    "        self.actv1 = nn.LeakyReLU()      \n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=(4,4),\n",
    "                                    stride=(4,4))  #output_shape 46 X 46 X 128     \n",
    "        self.dropout1 = nn.Dropout(p=dor1)\n",
    "        \n",
    "\n",
    "        #---------------------------------------------------------------------------------------\n",
    "\n",
    "        # Set 2\n",
    "        out_channels2 = 128\n",
    "        # conv --> BN --> LeakyRelu --> Pool --> DO\n",
    "        self.conv2 = nn.Conv2d(in_channels = out_channels1,\n",
    "                              out_channels = out_channels2,\n",
    "                              kernel_size = (3,3)) #output_shape 44, 44, 128\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels2)\n",
    "        self.actv2 = nn.LeakyReLU()\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=(2,2),\n",
    "                                    stride=(2,2))  #output_shape 22 X 22 X 128\n",
    "        self.dropout2 = nn.Dropout(p=dor2)\n",
    "\n",
    "        #---------------------------------------------------------------------------------------\n",
    "\n",
    "        # Set 3\n",
    "        out_channels3 = 256\n",
    "        # conv --> BN --> LeakyRelu --> Pool --> DO\n",
    "        self.conv3 = nn.Conv2d(in_channels = out_channels2,\n",
    "                              out_channels = out_channels3,\n",
    "                              kernel_size = (3,3)) #output_shape 20, 20, 256\n",
    "        self.bn3 = nn.BatchNorm2d(out_channels3)\n",
    "        self.actv3 = nn.LeakyReLU()\n",
    "        self.maxpool3 = nn.MaxPool2d(kernel_size=(2,2),\n",
    "                                    stride=(2,2))  #output_shape 10 X 10 X 256\n",
    "        self.dropout3 = nn.Dropout(p=dor3)\n",
    "\n",
    "        #---------------------------------------------------------------------------------------\n",
    "\n",
    "        # Set 4\n",
    "        out_channels4 = 256\n",
    "        # conv --> BN --> LeakyRelu --> Pool --> DO\n",
    "        self.conv4 = nn.Conv2d(in_channels = out_channels3,\n",
    "                              out_channels = out_channels4,\n",
    "                              kernel_size = (3,3)) #output_shape 8, 8, 512\n",
    "        self.bn4 = nn.BatchNorm2d(out_channels4)\n",
    "        self.actv4 = nn.LeakyReLU()\n",
    "        self.maxpool4 = nn.MaxPool2d(kernel_size=(2,2),\n",
    "                                    stride=(2,2))  #output_shape 4 X 4 X 512\n",
    "        self.dropout4 = nn.Dropout(p=dor4)\n",
    "\n",
    "        #---------------------------------------------------------------------------------------\n",
    "\n",
    "        # Head\n",
    "        out_channels5 = 1024\n",
    "        # Linear --> BN --> LeakyRelu --> Pool --> DO\n",
    "        self.fc1 = nn.Linear(in_features = 4 * 4 * out_channels4,\n",
    "                              out_features = out_channels5) #output_shape = 1024\n",
    "        self.bn5 = nn.BatchNorm1d(out_channels5)\n",
    "        self.actv5 = nn.LeakyReLU()\n",
    "        self.dropout5 = nn.Dropout(p=dor5)\n",
    "\n",
    "\n",
    "        # final output layer\n",
    "        self.fc2 = nn.Linear(in_features = out_channels5,\n",
    "                              out_features = classes) #output_shape = 1024\n",
    "        self.logSoftmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        #pass through 1\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.actv1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        #pass through 2\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.actv2(x)\n",
    "        x = self.maxpool2(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "\n",
    "        #pass through 3\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.actv3(x)\n",
    "        x = self.maxpool3(x)\n",
    "        x = self.dropout3(x)\n",
    "\n",
    "        #pass through 4\n",
    "        x = self.conv4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.actv4(x)\n",
    "        x = self.maxpool4(x)\n",
    "        x = self.dropout4(x)\n",
    "\n",
    "        x = torch.flatten(x,1) #flattening the layers\n",
    "\n",
    "        #head fully connected layers\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn5(x)\n",
    "        x = self.actv5(x)\n",
    "        x = self.dropout5(x)\n",
    "\n",
    "        #output\n",
    "        x = self.fc2(x)\n",
    "        return self.logSoftmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231f6339-64c9-4537-ac65-1d478eda1e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "numChannels = 3\n",
    "\n",
    "model = LeNet(numChannels = numChannels, classes=numClasses).to(device)\n",
    "\n",
    "summary(model, (numChannels, IMG_HEIGHT, IMG_WIDTH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fe09fe-fd2b-409e-9d9e-ceaff88b44a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inittialize out optimizer and loss function\n",
    "optimizer = torch.optim.AdamW(model.parameters(),\n",
    "                             lr = ALPHA)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss() # cross entropy loss\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
    "                                                         factor = LR_FACTOR,\n",
    "                                                         patience=LR_PATIENCE,\n",
    "                                                         min_lr=1e-5)\n",
    "H = {\n",
    "    'Epoch' : [],\n",
    "    'Train Loss' : [],\n",
    "    'Test Loss' : [],\n",
    "    'Train Acc' : [],\n",
    "    'Test Acc' : [],\n",
    "    'Alpha' : [],\n",
    "    'Patience' : []\n",
    "}\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd04f509-3333-41aa-a0c3-e79a148bf3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Min Loss\n",
    "minLoss = float('inf')\n",
    "#path to save the model\n",
    "savePath = os.path.join(modelDir, subDir, 'flowers.pth')\n",
    "\n",
    "#train the model for multiple epochs\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    #timer\n",
    "    epochStart = time.time()\n",
    "\n",
    "    #training loop\n",
    "    train_loss = 0.0\n",
    "    train_acc = 0.0\n",
    "\n",
    "    for i, data in enumerate(train_dataloader):\n",
    "        model.train()\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        batch_loss = loss_fn(outputs, labels)\n",
    "        batch_acc = accuracy_score(labels.cpu().numpy(), preds.cpu().numpy())\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += batch_loss.item() * inputs.size(0)\n",
    "        train_acc += batch_acc * inputs.size(0)\n",
    "\n",
    "    train_loss /= len(train_dataset)\n",
    "    train_acc /= len(train_dataset)\n",
    "\n",
    "    H['Train Loss'].append(train_loss)\n",
    "    H['Train Acc'].append(train_acc)\n",
    "\n",
    "    # Testing loop\n",
    "    test_loss = 0.0\n",
    "    test_acc = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "\n",
    "        for data in test_dataloader:\n",
    "\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "  \n",
    "            preds = torch.argmax(outputs, dim = 1)\n",
    "\n",
    "            batch_loss = loss_fn(outputs, labels)\n",
    "\n",
    "            batch_acc = accuracy_score(labels.cpu().numpy(), preds.cpu().numpy())\n",
    "\n",
    "            test_loss += batch_loss.item() * inputs.size(0)\n",
    "            test_acc += batch_acc* inputs.size(0)\n",
    "\n",
    "        test_loss /= len(test_dataset)\n",
    "        test_acc /= len(test_dataset)\n",
    "\n",
    "        H['Test Loss'].append(test_loss)\n",
    "        H['Test Acc'].append(test_acc)\n",
    "        \n",
    "    H['Epoch'].append(epoch)\n",
    "\n",
    "    #Step schedulee\n",
    "    lr_scheduler.step(test_loss)\n",
    "\n",
    "    ## add early stopping/ saving the model\n",
    "    if test_loss < minLoss:\n",
    "        minLoss = test_loss\n",
    "        counter = 0\n",
    "        #saving model\n",
    "        torch.save({ 'epoch': epoch+1,\n",
    "                   'model_state_dict': model.state_dict(),\n",
    "                   'optimizer_state_dict': optimizer.state_dict(),\n",
    "                   'loss': loss_fn,}, savePath )\n",
    "    else:\n",
    "        counter +=1\n",
    "    if counter > PATIENCE:\n",
    "        break\n",
    "    #----------------------------------\n",
    "    curr_alpha = lr_scheduler.get_last_lr()[0]\n",
    "    H['Alpha'].append(curr_alpha)\n",
    "    H['Patience'].append(counter)\n",
    "\n",
    "    if epoch%5 == 0:\n",
    "        #print the model training and validation information\n",
    "        print(f' Epoch {epoch:3d}/{EPOCHS:3d}', end= ' ') \n",
    "        print(f' Loss  {train_loss:.6f}/{test_loss:.6f}, Accuracy: {train_acc:.4f}/{test_acc:.4f}', end= ' ')\n",
    "        print(f' Alpha {curr_alpha:.6f}, ES: {counter:2d}', end= ' ')\n",
    "        print(f' Epoch time: {time.time() - epochStart:.2f} Sec')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3904578c-0fc1-479a-a9ef-bda538ca9b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_plot_torch_hist(loss_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e49db1b-e2ab-4711-98ee-490d888a27b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
